##########
# Import #
##########
import sys
from typing import Union

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


from keras.models import Sequential, load_model
from keras.layers import (
    Dense,
    Input,
    Conv1D,
    MaxPooling1D,
    Dropout,
    Flatten,
    Activation,
    BatchNormalization,
)
from keras.callbacks import EarlyStopping, Callback, ModelCheckpoint
from keras.optimizers import adam_v2

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.feature_extraction import FeatureHasher

###############
# Import Util #
###############
sys.path.insert(0, "./../")
from util import *


#############
# Functions #
#############
class EachModel:
    def __init__(self, make_model, name: str) -> None:
        self.name = name
        self.build_model = make_model

    def make_model(self, input_shape: tuple) -> None:
        self.input_shape = input_shape
        self.model = self.build_model(input_shape)

    def predict(self, x_test) -> np.ndarray:
        x_test = x_test.reshape((len(x_test),) + self.input_shape)
        return self.model.predict(x_test)

    def evaluate(self, x_test, y_test) -> None:
        x_test = x_test.reshape((len(x_test),) + self.input_shape)
        loss, acc = self.model.evaluate(x_test, y_test)

        print(
            f"""
        Result
            - Loss : %3.8f
            - Accuracy : %3.8f %%
        """
            % (loss, acc * 100)
        )

        return loss, acc

    def render_history(self, history, loss: float = None, acc: float = None) -> None:
        plt.figure(figsize=(30, 9))
        plt.subplot(1, 2, 1)
        plt.plot(
            range(len(history.history["loss"])), history.history["loss"], label="loss"
        )
        plt.plot(
            range(len(history.history["val_loss"])),
            history.history["val_loss"],
            label="val_loss",
        )
        if loss is not None:
            plt.axhline(loss, color="red", label="loss of testset")
        plt.xlabel("Change of Loss")
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(
            range(len(history.history["binary_accuracy"])),
            history.history["binary_accuracy"],
            label="accuracy for training",
        )
        plt.plot(
            range(len(history.history["val_binary_accuracy"])),
            history.history["val_binary_accuracy"],
            label="accuracy for validation",
        )
        if acc is not None:
            plt.axhline(acc, color="red", label="accuracy of testset")
        plt.xlabel("Change of Accuracy")
        plt.legend()

        plt.show()

    ############
    # Training #
    ############
    def train(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int = 200):
        x_train = x_train.reshape((len(x_train),) + self.input_shape)

        checkpoint = ModelCheckpoint(
            f"{MAIN_DIR}/model/{self.name}.model",
            monitor="val_binary_accuracy",
            verbose=0,
            save_best_only=True,
            mode="max",
        )
        # 모델 구성 & 훈련
        history = self.model.fit(
            x_train,
            y_train,
            epochs=epochs,
            validation_split=0.2,
            verbose=0,
            callbacks=[checkpoint],
        )
        self.model = load_model(f"{MAIN_DIR}/model/{self.name}.model")
        return history

    def train_kfold(
        self,
        x_train: np.ndarray,
        y_train: np.ndarray,
        fold_num: int,
        epochs: int = 200,
    ):
        x_train = x_train.reshape((len(x_train),) + self.input_shape)

        checkpoint = ModelCheckpoint(
            f"{MAIN_DIR}/model/{self.name}.kfold.model",
            monitor="val_binary_accuracy",
            verbose=0,
            save_best_only=True,
            mode="max",
        )

        fold_num = min(fold_num, len(x_train)) if len(x_train) < 10000 else 0

        kfold = StratifiedKFold(n_splits=fold_num, shuffle=True)
        kfold_models = []
        kfold_accuracy = []

        # train
        for learning_iter, data in enumerate(kfold.split(x_train, y_train)):
            train_index, test_index = data
            x_train_fold = x_train[train_index]
            y_train_fold = y_train[train_index]
            x_test_fold = x_train[test_index]
            y_test_fold = y_train[test_index]

            # 모델 구성 & 훈련
            model = self.build_model(input_shape=self.input_shape)
            history = model.fit(
                x_train_fold,
                y_train_fold,
                epochs=100,
                validation_data=[x_test_fold, y_test_fold],
                verbose=0,
            )

            kfold_models.append(model)

            # predict
            evaluate = model.evaluate(x_test_fold, y_test_fold)
            predict = model.predict(x_test_fold)

            num0 = (y_test_fold.flatten() > 0.5).sum()
            test_result = [num0, len(x_test_fold) - num0]

            num0 = (predict.flatten() > 0.5).sum()
            predict_result = [num0, len(x_test_fold) - num0]

            # result
            print(
                f"""
        Learning Iteration : %d
            - Accuracy\t %3.8f %%
            - Loss\t %.8f
        """
                % (learning_iter + 1, evaluate[1] * 100, evaluate[0])
            )

            plt.figure(figsize=(30, 4))
            # distribution of both of answer and predict
            plt.subplot(1, 5, 1)
            plt.bar([0, 1], test_result, alpha=0.5, color="red", label="real")
            plt.bar([0, 1], predict_result, alpha=0.5, color="blue", label="predict")
            plt.legend()
            # distribution of answer
            plt.subplot(1, 5, 2)
            plt.bar([0, 1], test_result, alpha=0.5, color="red", label="real")
            plt.legend()
            # distribution of predict
            plt.subplot(1, 5, 3)
            plt.bar([0, 1], predict_result, alpha=0.5, color="blue", label="predict")
            plt.legend()
            # loss
            plt.subplot(1, 5, 4)
            plt.plot(
                range(len(history.history["loss"])),
                history.history["loss"],
                label="loss",
            )
            plt.plot(
                range(len(history.history["val_loss"])),
                history.history["val_loss"],
                label="val_loss",
            )
            plt.xlabel("Change of Loss")
            plt.legend()
            # accuracy
            plt.subplot(1, 5, 5)
            plt.plot(
                range(len(history.history["binary_accuracy"])),
                history.history["binary_accuracy"],
                label="accuracy for train",
            )
            plt.plot(
                range(len(history.history["val_binary_accuracy"])),
                history.history["val_binary_accuracy"],
                label="accuracy for validation",
            )
            plt.xlabel("Change of Accuracy")
            plt.legend()
            plt.show()

            kfold_accuracy.append(evaluate + [history.history["binary_accuracy"][-1]])

        # final train
        # pick best model
        kfold_accuracy = np.array(kfold_accuracy)
        model = kfold_models[np.argmax(kfold_accuracy[:, 1])]
        history = model.fit(
            x_train,
            y_train,
            epochs=epochs,
            validation_split=0.2,
            verbose=0,
            callbacks=[checkpoint],
        )
        self.model = load_model(f"{MAIN_DIR}/model/{self.name}.kfold.model")

        # 결과 확인
        print(
            "K-Fold average accuracy : %3.8f %%" % (kfold_accuracy[:, 1].mean() * 100)
        )
        plt.figure(figsize=(20, 9))
        plt.plot(kfold_accuracy[:, 0], label="Loss chages according to iterations")
        plt.plot(
            kfold_accuracy[:, 1], label="Change of accuracy in test according to iters"
        )
        plt.plot(
            kfold_accuracy[:, 2], label="Change of accuracy in train according to iters"
        )
        plt.axhline(
            y=kfold_accuracy[:, 1].mean(),
            label="Averaged accuracy of train",
            color="red",
        )
        plt.axhline(y=evaluate[1], label="Final accuracy of train", color="orange")
        plt.xlabel("Accuracy according to iters")
        plt.legend()
        plt.show()

        return history


class Model:
    def __init__(self, x_all, y_all) -> None:
        self.dnn = EachModel(self.__dnn_model, "dnn")
        self.cnn = EachModel(self.__cnn_model, "cnn")

        self.make_dataset(x_all=x_all, y_all=y_all)

    ############
    # Modeling #
    ############

    def __dnn_model(self, input_shape: tuple):
        model = Sequential()
        model.add(
            Dense(
                units=512,
                kernel_initializer="he_normal",
                activation="leaky_relu",
                input_shape=input_shape,
            )
        )
        # model.add(Dropout(0.2))
        model.add(
            Dense(units=256, kernel_initializer="he_normal", activation="leaky_relu")
        )
        # model.add(Dropout(0.2))
        model.add(
            Dense(units=128, kernel_initializer="he_normal", activation="leaky_relu")
        )
        # model.add(Dropout(0.2))
        model.add(
            Dense(units=128, kernel_initializer="he_normal", activation="leaky_relu")
        )
        # model.add(Dropout(0.2))
        model.add(
            Dense(units=64, kernel_initializer="he_normal", activation="leaky_relu")
        )
        model.add(Dense(units=1, kernel_initializer="he_normal", activation="sigmoid"))

        model.compile(
            loss="binary_crossentropy",
            optimizer=adam_v2.Adam(0.0001),
            metrics=["binary_accuracy"],
        )

        return model

    def __cnn_model(self, input_shape: tuple):
        model = Sequential()

        # Convolution
        model.add(Conv1D(3, 2, kernel_initializer="he_normal", input_shape=input_shape))
        # model.add(BatchNormalization())
        model.add(Activation("leaky_relu"))

        # model.add(Conv1D(3, 4, kernel_initializer='he_normal'))
        # # model.add(BatchNormalization())
        # model.add(Activation('leaky_relu'))

        model.add(Conv1D(3, 8, kernel_initializer="he_normal"))
        # model.add(BatchNormalization())
        model.add(Activation("leaky_relu"))

        # model.add(Conv1D(3, 16, kernel_initializer='he_normal'))
        # # model.add(BatchNormalization())
        # model.add(Activation('leaky_relu'))

        model.add(MaxPooling1D(2))
        model.add(Flatten())

        # FC
        model.add(Dense(units=512, kernel_initializer="he_normal"))
        # model.add(BatchNormalization())
        model.add(Activation("leaky_relu"))
        # model.add(Dropout(0.2))

        model.add(Dense(units=256, kernel_initializer="he_normal"))
        # model.add(BatchNormalization())
        model.add(Activation("leaky_relu"))
        # model.add(Dropout(0.2))

        model.add(Dense(units=128, kernel_initializer="he_normal"))
        # model.add(BatchNormalization())
        model.add(Activation("leaky_relu"))
        # model.add(Dropout(0.2))

        model.add(Dense(units=128, kernel_initializer="he_normal"))
        # model.add(BatchNormalization())
        model.add(Activation("leaky_relu"))
        # model.add(Dropout(0.2))

        model.add(Dense(units=64, kernel_initializer="he_normal"))
        # model.add(BatchNormalization())
        model.add(Activation("leaky_relu"))
        # model.add(Dropout(0.2))

        # model.add(Dense(units=16, kernel_initializer='he_normal'))
        # # model.add(BatchNormalization())
        # model.add(Activation('leaky_relu'))

        # output layer
        model.add(Dense(units=1, activation="sigmoid"))

        model.compile(
            loss="binary_crossentropy",
            optimizer=adam_v2.Adam(0.0001),
            metrics=["binary_accuracy"],
        )

        return model

    ###########
    # Dataset #
    ###########

    def make_dataset(
        self,
        x_all: Union[np.ndarray, pd.DataFrame] = None,
        y_all: Union[np.ndarray, pd.DataFrame] = None,
        x_train: Union[np.ndarray, pd.DataFrame] = None,
        y_train: Union[np.ndarray, pd.DataFrame] = None,
        x_test: Union[np.ndarray, pd.DataFrame] = None,
        y_test: Union[np.ndarray, pd.DataFrame] = None,
    ) -> list:
        """
        make dataset

        Return
        ------
        - list: [x-trainset, y-trainset, x-testset, y-testset]
        """

        ERR_MSG = """Invalid Usage : parameters should be passed
        as a pair of (x_all, y_all) or (x_train, y_train, x_test, y_test)"""

        if x_all is not None:
            if y_all is None:
                print(ERR_MSG, file=sys.stderr)
                raise ValueError

            if type(x_all) == pd.DataFrame:
                x_all = x_all.values
            if type(y_all) == pd.DataFrame:
                y_all = y_all.values

            x_train, x_test, y_train, y_test = train_test_split(
                x_all, y_all, test_size=0.2, random_state=123, stratify=y_all
            )

        elif x_train is not None:
            if y_train is None or x_test is None or y_test is None:
                print(ERR_MSG, file=sys.stderr)
                raise ValueError

            if type(x_train) == pd.DataFrame:
                x_train = x_train.values
            if type(y_train) == pd.DataFrame:
                y_train = y_train.values
            if type(x_test) == pd.DataFrame:
                x_test = x_test.values
            if type(y_test) == pd.DataFrame:
                y_test = y_test.values

        else:
            print(ERR_MSG, file=sys.stderr)
            raise ValueError

        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test

        return x_train, y_train, x_test, y_test

    #########
    # Train #
    #########

    def dnn_train(
        self,
        input_shape: tuple,
        epochs: int = 200,
        kfold: bool = True,
        kfold_num: int = 5,
        render_result: bool = True,
    ):
        self.dnn.make_model(input_shape)

        if kfold:
            history = self.dnn.train_kfold(
                self.x_train, self.y_train, kfold_num, epochs
            )
        else:
            history = self.dnn.train(self.x_train, self.y_train, epochs)

        loss, acc = self.dnn.evaluate(self.x_test, self.y_test)

        if render_result:
            self.dnn.render_history(history, loss, acc)

    def cnn_train(
        self,
        input_shape: tuple,
        epochs: int = 200,
        kfold: bool = True,
        kfold_num: int = 5,
        render_result: bool = True,
    ):
        self.cnn.make_model(input_shape)

        if kfold:
            history = self.cnn.train_kfold(
                self.x_train, self.y_train, kfold_num, epochs
            )
        else:
            history = self.cnn.train(self.x_train, self.y_train, epochs)

        loss, acc = self.cnn.evaluate(self.x_test, self.y_test)

        if render_result:
            self.cnn.render_history(history, loss, acc)
