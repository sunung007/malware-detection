##########
# Import #
##########
import sys
from typing import Union

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


from keras import backend as K
from keras.models import Sequential, load_model
from keras.layers import (
    Dense,
    Input,
    Conv1D,
    MaxPooling1D,
    Dropout,
    Flatten,
    Activation,
    BatchNormalization,
)
from keras.callbacks import EarlyStopping, Callback, ModelCheckpoint
from keras.optimizers import adam_v2

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.feature_extraction import FeatureHasher

###############
# Import Util #
###############
sys.path.insert(0, "./../")
from util import *


#############
# Functions #
#############
class EachModel:
    def __init__(self, make_model, name: str) -> None:
        self.name = name
        self.build_model = make_model

    def make_model(self, input_shape: tuple) -> None:
        self.input_shape = input_shape
        self.model = self.build_model(input_shape)

    def predict(self, x_test) -> np.ndarray:
        x_test = x_test.reshape((len(x_test),) + self.input_shape)
        return self.model.predict(x_test)

    def evaluate(self, x_test, y_test) -> None:
        x_test = x_test.reshape((len(x_test),) + self.input_shape)
        loss, acc = self.model.evaluate(x_test, y_test)

        print(
            f"""
        Result
            - Loss : %3.8f
            - Accuracy : %3.8f %%
        """
            % (loss, acc * 100)
        )

        return loss, acc

    def render_history(self, history, loss: float = None, acc: float = None) -> None:
        plt.figure(figsize=(30, 9))
        plt.subplot(1, 2, 1)
        plt.plot(
            range(len(history.history["loss"])), history.history["loss"], label="loss"
        )
        plt.plot(
            range(len(history.history["val_loss"])),
            history.history["val_loss"],
            label="val_loss",
        )
        if loss is not None:
            plt.axhline(loss, color="red", label="loss of testset")
        plt.xlabel("Change of Loss")
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(
            range(len(history.history["binary_accuracy"])),
            history.history["binary_accuracy"],
            label="accuracy for training",
        )
        plt.plot(
            range(len(history.history["val_binary_accuracy"])),
            history.history["val_binary_accuracy"],
            label="accuracy for validation",
        )
        if acc is not None:
            plt.axhline(acc, color="red", label="accuracy of testset")
        plt.xlabel("Change of Accuracy")
        plt.legend()

        plt.show()

    ############
    # Training #
    ############

    def train(
        self,
        name: str,
        make_model,
        input_shape,
        x_train,
        x_test,
        y_train,
        y_test,
        epochs: int = 100,
        fold_num: int = 5,
        fold_epochs: int = 100,
        batch_size: int = None,
    ):
        print(
            f"""
    Shape of input data
        - train : {x_train.shape}
        - test : {x_test.shape}
        ------------------------
        - input shape : {input_shape}
        """
        )

        fold_num = min(fold_num, len(x_train)) if len(x_train) < 10000 else 0

        ##########
        # K-fold #
        ##########

        if fold_num > 0:
            checkpoint = ModelCheckpoint(
                f"{MAIN_DIR}/model/{name}.kfold.model",
                monitor="custom_f1",
                verbose=0,
                save_best_only=True,
                mode="max",
            )

            kfold = StratifiedKFold(n_splits=fold_num, shuffle=True)
            kfold_models = []
            kfold_accuracy = []

            for learning_iter, data in enumerate(kfold.split(x_train, y_train)):
                ############
                # training #
                ############

                train_index, valid_index = data
                x_train_fold = x_train[train_index]
                y_train_fold = y_train[train_index]
                x_test_fold = x_train[valid_index]
                y_test_fold = y_train[valid_index]

                # 모델 구성 & 훈련
                model = make_model(
                    input_shape=input_shape, use_batch=(batch_size is not None)
                )
                history = model.fit(
                    x_train_fold,
                    y_train_fold,
                    epochs=fold_epochs,
                    batch_size=batch_size,
                    validation_split=0.2,
                    verbose=0,
                    callbacks=[checkpoint],
                )
                model = load_model(
                    f"{MAIN_DIR}/model/{name}.kfold.model",
                    custom_objects={"custom_f1": self.custom_f1},
                )
                kfold_models.append(model)

                ##########
                # result #
                ##########
                evaluate = model.evaluate(x_test_fold, y_test_fold)
                # train f1 score, train accuracy,
                # test loss, test f1 score, test accuracy
                kfold_accuracy.append(evaluate)

                print(
                    f"""
            Learning Iteration : %d
                - Loss of test fold\t %.8f
                - F1 score of test fold\t %3.8f
                - Accuracy of test fold\t %3.8f
            """
                    % (learning_iter + 1, evaluate[0], evaluate[1], evaluate[2])
                )

                predict = model.predict(x_test_fold)
                test_result = [(y_test_fold.flatten() > 0.5).sum(), len(x_test_fold)]
                test_result[1] -= test_result[0]
                predict_result = [(predict.flatten() > 0.5).sum(), len(x_test_fold)]
                predict_result[1] -= predict_result[0]

                # graph
                plt.figure(figsize=(40, 4))

                plt.subplot(1, 6, 1)
                plt.bar([0, 1], test_result, alpha=0.5, color="red", label="real")
                plt.bar(
                    [0, 1], predict_result, alpha=0.5, color="blue", label="predict"
                )
                plt.legend()

                plt.subplot(1, 6, 2)
                plt.bar([0, 1], test_result, alpha=0.5, color="red", label="real")
                plt.legend()

                plt.subplot(1, 6, 3)
                plt.bar(
                    [0, 1], predict_result, alpha=0.5, color="blue", label="predict"
                )
                plt.legend()

                plt.subplot(1, 6, 4)
                plt.plot(
                    range(len(history.history["loss"])),
                    history.history["loss"],
                    label="loss",
                )
                plt.plot(
                    range(len(history.history["val_loss"])),
                    history.history["val_loss"],
                    label="val_loss",
                )
                plt.axhline(y=evaluate[0], label="loss of test")
                plt.xlabel("Change of Loss")
                plt.legend()

                plt.subplot(1, 6, 5)
                plt.plot(
                    range(len(history.history["custom_f1"])),
                    history.history["custom_f1"],
                    label="f1 score for train",
                )
                plt.plot(
                    range(len(history.history["val_custom_f1"])),
                    history.history["val_custom_f1"],
                    label="f1 score for validation",
                )
                plt.axhline(y=evaluate[1], label="f1 score of test")
                plt.xlabel("Change of F1 score")
                plt.legend()

                plt.subplot(1, 6, 6)
                plt.plot(
                    range(len(history.history["binary_accuracy"])),
                    history.history["binary_accuracy"],
                    label="accuracy for train",
                )
                plt.plot(
                    range(len(history.history["val_binary_accuracy"])),
                    history.history["val_binary_accuracy"],
                    label="accuracy for validation",
                )
                plt.axhline(y=evaluate[2], label="accuracy of test")
                plt.xlabel("Change of Accuracy")
                plt.legend()

                plt.show()

            ###############
            # Final train #
            ###############

            # pick best model
            kfold_accuracy = np.array(kfold_accuracy)
            model = kfold_models[np.argmax(kfold_accuracy[:, 1])]  # f1 score 기준

            # final training
            history = model.fit(
                x_train,
                y_train,
                epochs=epochs,
                batch_size=batch_size,
                validation_split=0.2,
                verbose=0,
                callbacks=[checkpoint],
            )
            model = load_model(
                f"{MAIN_DIR}/model/{name}.kfold.model",
                custom_objects={"custom_f1": self.custom_f1},
            )

            # 결과 확인
            evaluate = model.evaluate(x_test, y_test)
            print(
                f"""
        K-Fold Result
            - batch size : {batch_size}
            - Average loss : %3.8f
            - Average f1 score : %3.8f
            - Average accuracy : %3.8f
            - Final test loss : %3.8f
            - Final test f1 score : %3.8f
            - Final test accuracy : %3.8f
        """
                % (
                    kfold_accuracy[:, 0].mean(),
                    kfold_accuracy[:, 1].mean(),
                    kfold_accuracy[:, 2].mean(),
                    evaluate[0],
                    evaluate[1],
                    evaluate[2],
                )
            )

            plt.figure(figsize=(20, 9))
            plt.plot(kfold_accuracy[:, 0], label="Loss")
            plt.plot(kfold_accuracy[:, 1], label="F1 score")
            plt.plot(kfold_accuracy[:, 2], label="Accuracy")
            plt.axhline(y=kfold_accuracy[:, 0].mean(), label="Averaged loss")
            plt.axhline(y=kfold_accuracy[:, 1].mean(), label="Averaged f1 score")
            plt.axhline(y=kfold_accuracy[:, 2].mean(), label="Averaged accuracy")
            plt.axhline(y=evaluate[0], label="Final loss of train")
            plt.axhline(y=evaluate[1], label="Final f1 score of train")
            plt.axhline(y=evaluate[2], label="Final accuracy of train")
            plt.xlabel("Accuracy according to iters")
            plt.legend()
            plt.show()

        ##############
        # Not K-fold #
        ##############
        else:
            checkpoint = ModelCheckpoint(
                f"{MAIN_DIR}/model/{name}.model",
                monitor="custom_f1",
                verbose=0,
                save_best_only=True,
                mode="max",
            )

            # 모델 구성 & 훈련
            model = make_model(input_shape=input_shape)
            history = model.fit(
                x_train,
                y_train,
                batch_size=batch_size,
                epochs=epochs,
                validation_split=0.2,
                verbose=0,
                callbacks=[checkpoint],
            )
            model = load_model(
                f"{MAIN_DIR}/model/{name}.model",
                custom_objects={"custom_f1": self.custom_f1},
            )

        return model, history


class Model:
    def __init__(self, x_all, y_all) -> None:
        self.dnn = EachModel(self.__dnn_model, "dnn")
        self.cnn = EachModel(self.__cnn_model, "cnn")

        self.make_dataset(x_all=x_all, y_all=y_all)

    def custom_f1(self, y_true, y_pred):
        def recall_m(y_true, y_pred):
            TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
            Positives = K.sum(K.round(K.clip(y_true, 0, 1)))

            recall = TP / (Positives + K.epsilon())
            return recall

        def precision_m(y_true, y_pred):
            TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
            Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))

            precision = TP / (Pred_Positives + K.epsilon())
            return precision

        precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)

        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))

    ############
    # Modeling #
    ############

    def __dnn_model(self, input_shape: tuple, use_batch=False):
        units = [4096, 2048, 512, 128]

        model = Sequential()
        for i in range(len(units)):
            # linear
            if i == 0:
                model.add(
                    Dense(
                        units=units[i],
                        kernel_initializer="he_normal",
                        input_shape=input_shape,
                    )
                )
            else:
                model.add(
                    Dense(
                        units=units[i],
                        kernel_initializer="he_normal",
                    )
                )
            # batch normlization
            if use_batch:
                model.add(BatchNormalization())
            # activation
            model.add(Activation("leaky_relu"))
            # dropout
            if i < len(units) - 5:
                model.add(Dropout(0.4))
            elif i < len(units) - 3:
                model.add(Dropout(0.2))

        model.add(Dense(units=1, kernel_initializer="he_normal", activation="sigmoid"))
        model.compile(
            loss="binary_crossentropy",
            optimizer=adam_v2.Adam(),
            metrics=[self.custom_f1, "binary_accuracy"],
        )

        return model

    def __cnn_model(self, input_shape: tuple, use_batch: bool = False):
        model = Sequential()

        conv_units = [2, 4]
        fc_units = [4098, 256, 128, 64]

        # Convolution
        for i in range(len(conv_units)):
            if i == 0:
                model.add(
                    Conv1D(
                        3,
                        conv_units[i],
                        kernel_initializer="he_normal",
                        input_shape=input_shape,
                    )
                )
            else:
                model.add(Conv1D(3, conv_units[i], kernel_initializer="he_normal"))
            if use_batch:
                model.add(BatchNormalization())
            model.add(Activation("leaky_relu"))
            model.add(Dropout(0.2))
            model.add(MaxPooling1D(2))

        model.add(Flatten())

        for i in range(len(fc_units)):
            if i == 0:
                model.add(
                    Dense(
                        units=fc_units[i],
                        kernel_initializer="he_normal",
                        input_shape=input_shape,
                    )
                )
            else:
                model.add(Dense(units=fc_units[i], kernel_initializer="he_normal"))
            if use_batch:
                model.add(BatchNormalization())
            model.add(Activation("leaky_relu"))

            if i < len(fc_units) - 3:
                model.add(Dropout(0.2))

        model.add(Dense(units=1, kernel_initializer="he_normal", activation="sigmoid"))
        model.compile(
            loss="binary_crossentropy",
            optimizer=adam_v2.Adam(0.0001),
            metrics=[self.custom_f1, "binary_accuracy"],
        )

        return model

    ###########
    # Dataset #
    ###########

    def make_dataset(
        self,
        x_all: Union[np.ndarray, pd.DataFrame] = None,
        y_all: Union[np.ndarray, pd.DataFrame] = None,
        x_train: Union[np.ndarray, pd.DataFrame] = None,
        y_train: Union[np.ndarray, pd.DataFrame] = None,
        x_test: Union[np.ndarray, pd.DataFrame] = None,
        y_test: Union[np.ndarray, pd.DataFrame] = None,
    ) -> list:
        """
        make dataset

        Return
        ------
        - list: [x-trainset, y-trainset, x-testset, y-testset]
        """

        ERR_MSG = """Invalid Usage : parameters should be passed
        as a pair of (x_all, y_all) or (x_train, y_train, x_test, y_test)"""

        if x_all is not None:
            if y_all is None:
                print(ERR_MSG, file=sys.stderr)
                raise ValueError

            if type(x_all) == pd.DataFrame:
                x_all = x_all.values
            if type(y_all) == pd.DataFrame:
                y_all = y_all.values

            x_train, x_test, y_train, y_test = train_test_split(
                x_all, y_all, test_size=0.2, random_state=123, stratify=y_all
            )

        elif x_train is not None:
            if y_train is None or x_test is None or y_test is None:
                print(ERR_MSG, file=sys.stderr)
                raise ValueError

            if type(x_train) == pd.DataFrame:
                x_train = x_train.values
            if type(y_train) == pd.DataFrame:
                y_train = y_train.values
            if type(x_test) == pd.DataFrame:
                x_test = x_test.values
            if type(y_test) == pd.DataFrame:
                y_test = y_test.values

        else:
            print(ERR_MSG, file=sys.stderr)
            raise ValueError

        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test

        return x_train, y_train, x_test, y_test

    #########
    # Train #
    #########

    def dnn_train(
        self,
        input_shape: tuple,
        epochs: int = 200,
        kfold: bool = True,
        kfold_num: int = 5,
        render_result: bool = True,
    ):
        self.dnn.make_model(input_shape)

        if kfold:
            history = self.dnn.train_kfold(
                self.x_train, self.y_train, kfold_num, epochs
            )
        else:
            history = self.dnn.train(self.x_train, self.y_train, epochs)

        loss, acc = self.dnn.evaluate(self.x_test, self.y_test)

        if render_result:
            self.dnn.render_history(history, loss, acc)

    def cnn_train(
        self,
        input_shape: tuple,
        epochs: int = 200,
        kfold: bool = True,
        kfold_num: int = 5,
        render_result: bool = True,
    ):
        self.make_dataset()

        cnn_model, cnn_history = train_model(
            name="cnn",
            make_model=self.cnn.make_model,
            input_shape=input_shape,
            x_train=x_train,
            x_test=x_test,
            y_train=y_train,
            y_test=y_test,
            epochs=50,
            fold_num=5,
            fold_epochs=50,
            batch_size=512,
        )
        show_result(
            cnn_model,
            cnn_history,
            x_test=x_test.reshape((len(x_test),) + cnn_input_shape),
            y_test=y_test,
        )

        if kfold:
            history = self.cnn.train_kfold(
                self.x_train, self.y_train, kfold_num, epochs
            )
        else:
            history = self.cnn.train(self.x_train, self.y_train, epochs)

        loss, acc = self.cnn.evaluate(self.x_test, self.y_test)

        if render_result:
            self.cnn.render_history(history, loss, acc)
